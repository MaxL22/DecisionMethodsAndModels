% !TeX spellcheck = en_US
\chapter{Weak rationality methods}
\label{ch:wrm}

\section{Partially inconsistent decision-makers}
\label{sec:partiallyinconsistent}

The process of MAUT is tedious, complex and error prone. In practice, the problem is that decision-makers are usually unable to estimate correctly the rates of substitution as required by the theory, because they also consider indifferent pairs of impact between which there is a slight preference. The result is that the computation of the weights $w_l$ based on $p-1$ pairs of indifferent impacts is unreliable.

We assume:
\begin{itemize}
	\item A preference relation $\Pi$ that is a weak order with an additive utility
	
	\item A certain environment: $|\Omega| = 1 \implies f(x, \omega)$ reduces to $f(x)$
	
	\item A single decision maker: $|D| = 1 \implies \Pi_d$ reduces to $\Pi$
\end{itemize}

However: 
\begin{itemize}
	\item The pairwise comparison matrix $\tilde \Lambda$ is incorrect
	
	\item The normalized utilities $\mu u(f_l)$ are incorrect
\end{itemize}
Therefore, $u(f) = \sum_{l \in P} w_l \tilde u_l (f_l)$ makes no sense-

The normalized utilities $\tilde u _l (x)$ and the pairwise comparison coefficients $\tilde \lambda_{lm}$ are only approximated ($\tilde \Lambda$ is typically positive and reciprocal, but inconsistent, giving nonunivocal weight vectors), and the approximations of $w_l$ and $\tilde u_l (x)$ combine in cascade.
$$ \tilde u(x) = \sum_{l \in P} \left(w_l \pm \delta w_l \right) \left(\tilde u_l (x) \pm \delta \tilde u_l (x)\right) $$

\section{Reconstructing consistent matrices}
\label{sec:reconstrconsistent}

A possible approach is to force consistency modifying $\tilde \Lambda$ as little as possible. This requires a definition of distance between matrices.

Given a metric $d: \R^{p^2} \times \R^{p^2} \rightarrow \R$, we can solve the problem
$$ \min d\left(W, \tilde \Lambda \right) $$
$$ W_{lm} = \frac{w_l}{w_m} $$
$$ w_l > 0, \quad l \in P, \quad \sum_{l \in P} w_l = 1$$
where the unknown variables are the weights $w_l$, $W$ denotes the matrix composed by the ratios $w_l/w_m$, $\tilde \Lambda$ the matrix denoted by the ratios estimated by the decision-maker and $d$ is a distance.

This is done in order to determine an unknown matrix $W$ that is 
\begin{itemize}
	\item positive, reciprocal and consistent
	
	\item as close as possible to the known matrix $\tilde \Lambda$
\end{itemize}

There are infinite possible definitions of distances, the choice is arbitrary, and the optimal matrix $W^\circ$ depends on the distance chosen. Examples: 
\begin{itemize}
	\item Manhattan distance $L_1$:
	$$ d_1 \left(W, \tilde \Lambda\right) = \sum_{l,m \in P} \left|\frac{w_l}{w_m} - \tilde \lambda_{lm} \right|$$
	
	\item Euclidean distance $L_2$:
	$$ d_2 \left(W, \tilde \Lambda\right) = \sqrt{\sum_{l,m \in P} \left(\frac{w_l}{w_m} - \tilde \lambda_{lm} \right)^2} $$
	
	\item $L_{\infty}$ distance:
	$$ d_{\infty} \left(W, \tilde \Lambda\right) = \max_{l,m \in P} \left|\frac{w_l}{w_m} - \tilde \lambda_{lm} \right| $$
\end{itemize}

\subsubsection{The eigenvalue method}

Another way to achieve consistency exploits the following properties: given a square matrix $\tilde \Lambda$ of order $p \times p$
\begin{itemize}
	\item eigenvalues are the $p$ solutions of equation $|\lambda l - \tilde \Lambda|$
	
	\item eigenvectors associated to $\lambda$ are the $\infty$ nonzero solutions of $\lambda x = \tilde \Lambda x$
\end{itemize}

If $\tilde \Lambda$ is positive, reciprocal and consistent
\begin{itemize}
	\item The dominant (maximum absolute value) eigenvalue is $\lambda_{\max} = p$
	
	\item The other $p-1$ eigenvalues are equal to zero ($\tilde \Lambda$ has rank 1)
	
	\item The eigenvectors associated to $\lambda_{\max}$ are proportional to the weight vector, and $w$ is the normalization of any dominant eigenvector $x_{\max}$
\end{itemize}

Consequently, the \textbf{eigenvalue method} proposes to
\begin{enumerate}
	\item Compute the eigenvalues and identify the dominant one $\lambda_{\max}$
	
	\item Compute the associated dominant eigenvector $x_{\max}$
	
	\item Normalize it to obtain the weight vector $w = \frac{x_{{\max}}}{\|x_{\max}\|}$
	
	\item Build the correct matrix as $W = \left\{\frac{w_l}{w_m}\right\}$
\end{enumerate}

But the resulting matrix $W$ can be far from $\tilde \Lambda$, other consistent matrices could be closer. Can a forcedly consistent matrix be safely used? This is subject of debate.

An alternative approach is to 
\begin{itemize}
	\item Consciously accept imprecise values for $w_l$ and $\tilde u_l (x)$ 
	
	\item Compute them based on the stronger aspects of human psychology
	
	\item Aim at a qualitative ranking, instead of a quantitative one
\end{itemize}

\section{Analytic Hierarchy Process AHP}
\label{sec:ahp}

The Analytic Hierarchy Process (AHP) by Saaty (1980): 
\begin{itemize}
	\item Replaces absolute measures with relative ones
	
	\item Replaces quantitative ratios with qualitative scales 
	
	\item Builds a hierarchy of indicators in order to compare only conceptually similar quantities
\end{itemize}

\subsection{Computation of the utilities with pairwise comparisons}

Given an indicator $f_l$, the classical method tries to reconstruct the whole absolute profile of a utility function $\tilde u_l (f_l)$, but psychology says that it's difficult for a human decision-maker to associate quantitative utility values to the values of the indicator.

The AHP focuses on pairs of solutions and on the strength of the relative preference between the values of an indicator in the two solutions of the pair. A matrix $\Lambda_l = \left\{\lambda_{xy}^{(l)}\right\}$ is built, whose element $\lambda_{xy}^{(l)}$ is associated to a pair of alternatives $(x,y) \in X \times X$ and evaluates how much $f_l (x)$ is preferable to $f_l (y)$.

This process evaluates the strength ratio between given pairs of alternatives, i a similar fashion to a utility ratio $\tilde u_l (f_l (x)) / \tilde u_l (f_l (x))$.

Since it requires the explicit enumeration of all solution pairs, the AHP can be applied \textbf{only to finite problems} and with a small number of solutions, or to problems whose solution set has been preliminarily pruned reducing it to a small finite set.

\subsection{Qualitative scales}

The second idea of the AHP is to measure the preference between values not with a quantitative scale but with a qualitative one, denoted as \textit{Saaty's scale}:
\begin{itemize}
	\item $\lambda_{xy}^{(l)} = 1$ for $f_l (x)$ \textit{equally good} as $f_l (y)$
	
	\item $\lambda_{xy}^{(l)} = 3$ for $f_l (x)$ \textit{moderately better} than $f_l (y)$
	
	\item $\lambda_{xy}^{(l)} = 5$ for $f_l (x)$ \textit{strongly better} than $f_l (y)$
	
	\item $\lambda_{xy}^{(l)} = 7$ for $f_l (x)$ \textit{very strongly better} than $f_l (y)$
	
	\item $\lambda_{xy}^{(l)} = 9$ for $f_l (x)$ \textit{absolutely better} than $f_l (y)$
\end{itemize}

The weights $2$, $4$, $6$ and $8$ are used for intermediate evaluations. The values are arbitrary, but they derive from the psychological fact that human decision-makers can't discriminate between more that $5$ levels in a reliable way.

Moreover, the use of a qualitative scale allows to compare heterogeneous quantities not expressed in a quantitative way, translating verbal judgments into numerical values. The idea is to give for granted that the values used are only a rough approximation, automatically raising an alert against trusting them too much. 

Building a whole pairwise comparison matrix also allows to evaluate its inconsistency and to tune it \textit{a posteriori} in order to make it consistent, instead of forcing a fictitious consistency since the beginning, which could introduce errors. \\

\begin{definition}
	We denote as \textbf{evaluation matrix} a matrix $U = \{u_{xl}\}$ containing the evaluation $u_{xl}$ of each alternative $x \in X$ with respect to each indicator $l \in P$, obtained starting from the pairwise comparison matrix $\Lambda_l$.
\end{definition}

The same qualitative process is followed for the weight ratios $\tilde \lambda_{lm} = \frac{w_l}{w_m}$ for each $l,m \in P$:
\begin{itemize}
	\item Choose representative qualitative values $\lambda_{lm}$
	
	\item Impose consistency 
	
	\item Derive pseudoweights $w_l$ from the consistent matrix
\end{itemize}

\subsection{Hierarchical structuring of the attributes}

Humans are bad at comparing nonhomogeneous things, so the idea is to build an indicator tree and compare only siblings indicators:
\begin{itemize}
	\item The level of the leaves includes the elementary attributes
	
	\item At the upper levels summarizing attributes appear, progressively more general
	
	\item The root corresponds to a sort of general objective
\end{itemize}

The idea of this process has already been discussed in Chapter \ref{ch:cases}.

Instead of comparing $l$ and $m$ for each $l,m \in P$
\begin{itemize}
	\item Estimate weight ratio $\lambda_{lm}$ only when $l$ and $m$ have the same father
	
	\item Perform comparisons at all levels of the indicator tree, not only between indicators, but also between indicator groups
\end{itemize}
Replace a single big pairwise comparison matrix with many small ones.

The advantages of structuring the attributes into a hierarchy are:
\begin{itemize}
	\item Only homogeneous attributes are compared with each other
	
	\item Each subset of pairwise comparisons can be assigned to an expert decision-maker, specialized in a specific field, so as to obtain more meaningful indications
	
	\item The total number of pairwise comparisons strongly decreases
\end{itemize}

The fundamental disadvantage is that the pairwise comparisons at the level of the leaves consider concrete and well-defined attributes, whereas those at the upper levels consider abstract and general objectives, for which a quantitative measure does not even make sense.

\subsection{Hierarchical recomposition}

The weights are normalized within each group of children nodes, but are not comparable with those of other group. The tree structure, however, allows to progressively build the attribute weight vector scanning the tree level-by-level from the leaves up to the root.

At each level, one builds a pairwise comparison matrix between the children of the same node, and derives from it a vector of weights with the methods described above. The weights in each vector have sum equal to one, and describe the relative weight between each other. In order to compare such weights with those of the other nodes of the whole tree, it is enough to renormalize them so that their sum coincides with the weight of the father node, that is simply to multiply them by the weight of the father node.

Given the pseudoutilities $\tilde u_{lx}$ and the pseudoweights $w_l$: 
\begin{itemize}
	\item Combine them with convex combinations
	
	\item Combine the pseudoweights with products from root to leaf, this corresponds to normalizing each set of sibling nodes
	$$ u(x) = \sum_{l \in P} \prod_{\ell \in \gamma_l} w_\ell \tilde u_{lx} $$
\end{itemize}

\subsection{Rank reversal}

The main defect of the AHP is the phenomenon of \textbf{rank reversal}: the order of the alternatives substantially depends on what alternatives are present. This happens because evaluations are pairwise comparisons and not absolute utility values, and therefore depend on the values actually compared.

In the AHP adding or removing alternatives can change the ranking, and alternatives can be generated in different phases, modifying the feasible region $X$.

A result depending on $X$ is undesirable, because. 
\begin{itemize}
	\item $X$ is not always given a priori
	
	\item Modifying $X$ allows to manipulate the result
\end{itemize}

Unfortunately, all decision processes based on pairwise comparisons between alternatives suffer from rank reversal (e.g., sport tournaments).

\subsubsection{Absolute scales, or \textit{a priori} estimate method}

In order to avoid rank reversal, Saaty proposed a system using absolute scales and a priori estimates: 
\begin{itemize}
	\item Fix a (finite) set of absolute levels for each indicator
	
	\item Make pairwise comparisons on levels, instead of alternatives 
	
	\item Evaluate each alternative assigning it to a level for each indicator
\end{itemize}

In this way, the pseudoutilities refer to absolute levels, fixed once for all, and not depending on the alternatives. Basically, define a number of levels and assign a value among these levels to each indicator.

The use of absolute classes also allows:
\begin{itemize}
	\item Open decision processes, in which alternatives arrive gradually
	
	\item Very long decision processes, in which alternatives arriving at far away times cannot be compared significantly
	
	\item To end the process as soon an alternative reaches a satisfactory threshold 
\end{itemize}

However, the trick also introduces further approximations:
\begin{itemize}
	\item Rather different values can be flattened putting them into the same class 
	
	\item Very similar values can be strongly differentiated putting them into separate classes
\end{itemize}

% End L13, p220 notes

\section{ELECTRE methods}
\label{sec:electre}

The \textbf{ELECTRE methods} (\textit{ELimination Et Choix Traduisant la REalité}) start from a basic criticism of the assumption that the decision-maker is able to compare all pairs of impacts. A typical example of this criticism is the \textit{coffee paradox}: two cups of coffee containing $f$ sugar and $f + \epsilon$ sugar will be indistinguishable for sufficiently small $\epsilon$, and the latter cup will be indifferent with respect to a third one containing $f + 2 \epsilon$ sugar and so on. By transitivity, two cups at the end of the chain containing $f$ and $2f$ sugar must also be indifferent, but this is obviously false. The classical theory of utility avoids the paradox assuming that the decision-maker is able to discriminate each link of the chain.

Let us extend the concept to a multi-attribute framework, in order to introduce the solution proposed by the ELECTRE methods. Let us assume that the cup of coffee $A$ contains a superior quality coffee with one grain more than the ideal quantity of sugar, whereas the cup of coffee $B$ contains an average quality coffee with the ideal quantity of sugar. According to the approach of Pareto, the two cups would be incomparable. According to the classical multi-attribute utility theory, they can be compared by establishing a marginal rate of substitution between the
attributes "coffee quality" and "sugar quantity". The ELECTRE methods work in yet another way.

We assume:
\begin{itemize}
	\item A preference relation $\Pi$ that is a nontransitive generalization of Paretian preference
	
	\item A certain environment: $|\Omega| = 1 \implies f(x, \omega)$ reduces to $f(x)$
	
	\item A single decision maker: $|D| = 1 \implies \Pi_d$ reduces to $\Pi$
\end{itemize}

The idea here is to:
\begin{itemize}
	\item Apply the Paretian preference definition (indicators as costs/benefits)
	
	\item Extend it with additional impact pairs generated by simple criteria (not individually)
\end{itemize}
\textit{We'll lose transitivity.}

Paretian preference is: 
\begin{itemize}
	\item Very natural: many indicators can be turned into costs or benefits
	
	\item Irrealistically poor: it does not take indifference curves to choose between "incomparable" trips such as 
	$$ 
	\left[\begin{array}{c}
		60 \text{ min} \\ 1000 \text{ Euros}
	\end{array}\right]
	\text{ and }
	\left[\begin{array}{c}
		61 \text{ min} \\ 1 \text{ Euro}
	\end{array}\right]
	$$
\end{itemize}

We want
\begin{itemize}
	\item A richer preference: more comparability
	
	\item No number crunching: no combination of estimated values
\end{itemize}

ELECTRE methods extend Paretian preference with a concept called \textbf{outranking}.

They focus on finite problems
\begin{itemize}
	\item Impacts and alternatives correspond one-to-one
	
	\item Preference (on impacts) and dominance (on solutions) are interchangeable
\end{itemize}

\subsection{The outranking relation}

The ELECTRE methods enrich the Pareto preference relation by admitting that $A$ could be preferable to $B$ also when it's worse for some attributes, provided the difference does not exceed a threshold. This is denoted as \textbf{outranking relation} $S$. \\

\begin{definition}
	Given two impacts $f, f' \in F \subseteq \R^p$, we say that $f$ \textbf{outranks} $f'$ $f \wpref{s} f'$, based on the threshold $\epsilon_l \geq 0$ when impact $f$ is not exceedingly worse that $f'$ for all indicators $l \in P$ with respect to the thresholds
	$$ f \wpref{s} f' \Leftrightarrow f_l \geq f_l' - \epsilon_l, \ \forall l \in P $$
\end{definition}

Setting $\epsilon_l = 0$ for all $l \in P$ yields the Paretian preference. Since generally $\epsilon_l > 0$, the outranking relation is richer, as in it contains a larger set of pairs (higher values make the preference richer, $\epsilon_l = + \infty$ yields complete indifference). The Paretian preference therefore implies the outranking relation while the opposite does not hold.

$f \wpref{s} f'$ does no longer mean "exchanging $f$ with $f'$ is acceptable", but "\textit{exchanging $f$ and $f'$ is not a clear loss}" (even if all $f_l$ can worsen).

The outranking relation is clearly reflexive, but in general \textit{does not have the transitive property}.

\subsubsection{Properties}

The basic outranking relation is
\begin{itemize}
	\item \textbf{Reflexive:} $f_l \geq f_l'$ for all $l \in P \Leftrightarrow f \wpref{s} f'$
	
	\item \textbf{Noncomplete:} $\left[\begin{array}{c}
		0 \\ 1
	\end{array}\right] \bowtie \left[\begin{array}{c}
	1 \\ 0 
	\end{array}\right]$ for any threshold $\epsilon_1 < 1$ or $\epsilon_2 < 1$
	
	\item \textbf{Nontransitive:} given thresholds $\epsilon_1 = \epsilon_2 = 1$
	$$ 
	\left[ \begin{array}{c}
		0 \\ 1
	\end{array}\right] \wpref{s} \left[ \begin{array}{c}
	1 \\ 2
	\end{array}\right] \ \text{ and } \ \left[ \begin{array}{c}
	1 \\ 2
	\end{array}\right] \wpref{s} \left[ \begin{array}{c}
	2 \\ 3
	\end{array}\right]
	$$
	$$ \text{but } \left[ \begin{array}{c}
		0 \\ 1
	\end{array}\right] \not \wpref{s} \left[ \begin{array}{c}
	2 \\ 3
	\end{array}\right]$$
	
	\item \textbf{Nonantisymmetric:} given thresholds $\epsilon_1 = \epsilon_2 = 1$
	$$
	\left[ \begin{array}{c}
		0 \\ 1
	\end{array}\right] \wpref{s} \left[ \begin{array}{c}
	1 \\ 0
	\end{array}\right] \ \text{ and } \ \left[ \begin{array}{c}
	1 \\ 0
	\end{array}\right] \wpref{s} \left[ \begin{array}{c}
	0 \\ 1
	\end{array}\right]
	$$ 
	$$ \text{but } \left[ \begin{array}{c}
		0 \\ 1
	\end{array}\right] \neq \left[ \begin{array}{c}
	1 \\ 0
	\end{array}\right]$$
\end{itemize}

\subsection{Refinements of the outranking relation}

The above definition easily produces relations too weak and too similar to the Paretian one when the thresholds $\epsilon_l$ are small, or relations too rich when the thresholds are too large. In both cases, the relation gives little information to the decision-maker.

The definition can be refined by combining it with other conditions: the final outranking relation will include the pairs of impacts that verify all conditions. Additional information is added to filter out some impact pairs.

The additional information will use indicator values and indicator weights
\begin{itemize}
	\item Avoiding any numerical combination of the two information
	
	\item Always manipulating them separately
\end{itemize}

The conditions allow to take into account additional remarks, for example expressing the relative importance of the indicators through suitable weights $w_l$.

So, a second way to define the outranking relation is to:
\begin{enumerate}
	\item Assign weights to the indicators
	$$ w_l \geq 0, \quad \forall l \in P \text{ with } \sum_{l \in P} w_l = 1 $$
	
	\item Treat the indicators as electors
	
	\item Hold a weighted election between impact pairs
\end{enumerate}

For each pair of impacts $(f, f')$, it is possible to define the values: 
\begin{itemize}
	\item The sum of the weights for the attributes for which $f_l$ is better than $f_l '$
	$$ w_{f f'}^+ = \sum_{l \in P: f_l > f_l'} w_l $$
	
	\item The sum of the weights for the attributes for which $f_l$ is indifferent to $f_l '$
	$$ w_{f f'}^= = \sum_{l \in P: f_l = f_l'} w_l $$
	
	\item The sum of the weights for the attributes for which $f_l$ is worse than $f_l '$
	$$ w_{f f'}^- = \sum_{l \in P: f_l < f_l'} w_l $$
\end{itemize}

Thanks to the normalization assumption, the following property holds: 
$$ w_{ff'}^+ + w_{ff'}^= + w_{ff'}^- = \sum_{l \in P} w_l = 1, \quad \forall f, f' \in F $$

\subsubsection{Outranking conditions}

Given a set of conditions, the outranking relation is given by the pair of impact s that satisfy all of them, and is therefore obtained by intersecting the relations defined by each condition. Some of these conditions are the following:
\begin{enumerate}
	\item The \textbf{satisfaction of comparability thresholds}: impact $f$ is not much worse than impact $f'$ for all attributes: 
	$$ f \wpref{s_\epsilon} f' \Leftrightarrow f_l \geq f_l' - \epsilon_l, \quad \forall l \in P $$
	where $\epsilon_l \geq 0$ for all $l \in P$. This is the basic condition introduced above. As $\epsilon_l$ grows, ti includes other pairs of impacts without losing any Paretian pair
	
	\item The \textbf{concordance condition}: a subset of attributes of sufficient weight agree that $f$ is not worse than $f'$:
	$$ f \wpref{s_c} f' \Leftrightarrow c_{ff'} = w_{ff'}^+ + w_{ff'}^= \geq \alpha_c, \ \text{ with } \alpha_c \in [0; 1] $$
	For $\alpha_c = 1$ this yields the Paretian preference, since $w_{ff'}^- \leq 1 - \alpha_c = 0$. For smaller values of $\alpha_c$, the relation includes larger and larger subsets of pairs. The indicators that "vote" for $f$ being better than $f'$ provide a second outranking relation, through the user-defined concordance threshold $\alpha_c$, that indicates how much the indicators should support the outranking
	
	\item The \textbf{discordance condition}: no attribute rejects with exceeding strength the statement that $f$ is better than $f'$:
	$$ f \wpref{s_d} f' \Leftrightarrow d_{ff'} = \begin{cases}
		\frac{\max_{l \in P} \left[\max (f_l' - f_l, 0)\right]}{\max_{l \in P} \left|f_l - f_l'\right|} & (\text{for } f \neq f') \\
		0 & (\text{for } f = f')
	\end{cases} \leq 1 - \alpha_d \ \text{ with } \alpha_d \in [0;1] $$
	The numerator measures the maximum difference between the two impacts with respect to the indicators for which $f'$ is better, that is $f_' > f_l$. The denominator measures the overall maximum difference between the two impacts. The ratio varies between 0, when $f$ is never worse than $f'$, and 1, when $f$ is worse than $f'$ for the indicator with the maximum difference.This index expresses the regret that one would feel rejecting $f'$ in favor of $f$. For $\alpha_d = 1$, one obtains the Paretian preference, for other values the relation enlarges. Since different attributes are combined in the index, it's important that they are normalized in advance, in order to make the comparison independent from the units of measure adopted. This provides a third outranking relation, through the user-defined discordance threshold $\alpha_d$, that indicates how much the indicators can oppose the outranking at most
\end{enumerate}

Since the different definitions capture different aspects of the problem, we intersect the three definitions to refine the overall outranking relation
$$ S = S_\epsilon \cap S_c \cap S_d $$

The threshold parameters $\alpha_c$ and $\alpha_d$ are arbitrary. A reasonable tuning to obtain an intermediate number of impact pairs is the average of the coefficients with which they are compared:
$$ \alpha_c = \frac{\sum_{f \in F} \sum_{f' \in F \setminus \{f\}} c_{ff'}}{|F| (|F| - 1)} $$
$$ \alpha_d = 1 - \frac{\sum_{f \in F} \sum_{f' \in F \setminus \{f\}} d_{ff'}}{|F| (|F| - 1)} $$

But this way $S$ becomes dependent on the set of alternatives, making rank reversal possible.

\subsection{Kernel identification}

Once obtained, the outranking relation is used to filter solutions out, keeping into account that it is weaker than the Paretian preference: 
\begin{itemize}
	\item Not outranked solutions are certainly interesting
	
	\item Outranked solutions are not necessarily bad
	
	\item Solutions strictly outranked by good solutions are not very useful \\
\end{itemize}

\begin{definition}
	We denote as \textbf{kernel} the subset of alternatives obtained with the following procedure: 
	\begin{enumerate}
		\item Start with an empty kernel ($K _= \emptyset$)
		
		\item Add to the kernel the subset of all solutions not strictly outranked ($K := K \cup \{x \in X \mid \nexists x' \in X : x' \prec_S x \}$)
		
		\item Remove from $X$ all solutions outranked by a kernel solution ($X:= X \setminus \{x \in X \mid \exists x' \in X : x' \prec_S x \}$)
		
		\item If the reduced $X$ contains only the kernel, terminate; otherwise go back to step 2
	\end{enumerate}
\end{definition}

This directly suggests an algorithm to reduce $X$ to a kernel $K \subseteq X$ that provides a representative subset of solutions:
\begin{myalgo}{0.95}
	$K:=\emptyset$; \\
	\While{$K \subset X$}{
		\tcp{select all solutions not strictly outranked}
		$K := K \cup \{x \in X \mid \nexists x' \in X : x' \prec_S x \}$;\\
		\tcp{remove all solutions strictly outranked by the kernel}
		$X:= X \setminus \{x \in X \mid \exists x' \in X : x' \prec_S x \}$ \\
	}
	\Return{$K$;}
\end{myalgo}

\subsubsection{The case of cyclic graphs}

The definition of kernel has a strong intrinsic limitation: a kernel is not guaranteed to exist. If the outranking graph contains circuits the procedure does not terminate. It terminates when the cycles are broken (with the intervention of the decision-makers) by strict outranking.

Some solutions have been proposed: 
\begin{itemize}
	\item Tweak the thresholds $\epsilon_l$, $\alpha_c$, $\alpha_d$ (\textit{might be ineffective})
	
	\item Find a circuit not strictly outranked and merge it into a macronode (\textit{this always works, but might yield the whole feasible region})
\end{itemize}

\subsection{Creation of a weak ordering}

Some ELECTRE methods include a final phase in which additional criteria are introduced to sort the solutions of the kernel. As in the Paretian case, this requires to perform a partial stretching.

\subsubsection{Topological ordering}

The nodes with no ingoing arcs correspond to nonoutranked solutions, and as such they naturally stand out as the best ones, similarly, the nodes with no outgoing arcs stand out as the worst ones.

Any topological ordering of the graph respects the outranking relation, meaning that it sets the outranking nodes in position preceding the outranked ones. This requires an acyclic graph, so cycles have to be already removed. Such a graph is partially ordered.

The topological ordering introduces a stretching, because it selects one of the several total orders that are consistent with the original partial order. Notice that starting from the nodes with zero indegree and proceeding forward (\textbf{forward ordering}) produces a different result with respect to starting from the nodes with zero outdegree and proceeding backwards (\textbf{backward ordering}).

The forward approach proceeds as follows: 
\begin{enumerate}
	\item Define an empty list $L_f$
	
	\item Find an arbitrary solution $x \in X$ not strictly outranked in $S$
	
	\item Append $x$ to $L_f$
	
	\item Remove $x$ from $X$ and all arcs $(x,y)$ from $S$
	
	\item If the graph is nonempty go to step 2
	
	\item Return list $L_f$
\end{enumerate}

The backward approach proceeds similarly, but find solutions which do not outrank any other one and reverses the list $L_b$ before returning it.

The combined approach:
\begin{enumerate}
	\item Applies the forward approach, obtaining $L_f$
	
	\item Computes the Borda count on $L_f$, assigning to each solution the number of not preceding ones
	
	\item Applies the backward approach, obtaining $L_b$
	
	\item Computes the Borda count on $L_b$, assigning to each solution the number of not preceding ones
	
	\item Sums the two Borda counts to obtain a weak order
\end{enumerate}

\subsubsection{Ordering with aggregated indices}

This method builds for each impact a concordance and a discordance aggregated index, starting with those associated with the pairs of impacts: 
\begin{itemize}
	\item \textbf{Concordance index:} it tries to describe the satisfaction associated to the choice of an impact; its values are large when the impact prevails on the other ones for many weighty attributes, while the other impacts prevail on it for few light attributes
	$$ C_f = \sum_{g \in F} \left(c_{fg} - c_{gf} \right), \ f \in F $$
	
	\item \textbf{Discordance index:} it decreases when the regret for a victory of the impact is small, while the regret for the defeat is large
	$$ D_f = \sum_{g \in F} \left(d_{fg} - d_{gf} \right), \ f \in F $$
\end{itemize}

% End L14, p232 notes, before excercises